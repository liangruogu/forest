---
title: 高斯混合模型
published: 2024-11-28
updated: 2024-11-28
description: '本文主要介绍高斯混合模型，并通过解决一个具体的聚类问题来加深对概率论的理解，同时也将使用python对数据进行可视化使过程更加直观'
image: 'https://raw.githubusercontent.com/liangruogu/BlogImages/main/img/20250530154749106.png'
tags: ["math", "python", "ML"]
category: 'math'
draft: false 
---


## 引言

​	随着数据的快速增长，尤其是在图像、文本、金融和生物领域，无监督学习已经成为一种至关重要的技术。与传统的有监督学习不同，无监督学习无需依赖于标注数据，能够从大量未标注的数据中自动挖掘出有意义的结构和模式。特别是对于标注数据难以获取或成本高昂的情况，无监督学习显得尤为重要。

高斯混合模型（Gaussian Mixture Model, GMM）作为无监督学习中的一种常见方法，已广泛应用于聚类、密度估计、异常检测等领域。GMM通过假设数据来源于若干个高斯分布的混合模型，能够有效地对复杂数据进行建模。其最大的优势在于能够捕捉数据中的多模态分布结构，使得它在实际应用中比传统的单一分布模型更加灵活和高效。



## 高斯混合模型

### 变量规定

在建立模型之前，先规定以下符号

$C_k表示第 k种类别$

$d表示样本数据的维数$

$\mu_k表示第 k个高斯分布参数中的均值$

$\Sigma_k 表示第k个高斯分布参数中的协方差矩阵$

$\pi_k表示第k个高斯分布的权重$

$K 表示所有种类的数量$

$X表示所有样本数据$

$N表示样本数量$

$x^{(i)}表示第i个样本$

$Z表示隐变量$



### 模型建立

​	为了使这个过程更加通顺，可以借助一张随机变量的pdf图像来说明

![高斯混合模型（GMM）](https://pic1.zhimg.com/70/v2-bec8cb8d4ddee4ae698e2a4a41eb1610_1440w.image?source=172ae18b&biz_tag=Post)

红色的曲线是整体的分布曲线，它十分的复杂。但是我们可以尝试使用三个不同的高斯分布来拟合这个曲线，像那三条蓝色的曲线一样。这样我们就可以很直观的理解高斯混合模型的本质了--使用多个高斯分布来拟合一个较为复杂的分布。我们这样做到原因也很简单，假设一个未知的随机变量服从高斯分布这件事几乎理所当然（同时推荐你看看我写的另一篇关于高斯分布的文章），这是因为中心极限定理。但是有时候，如果我们直接假设它服从高斯分布得到的结果可能不会很理想，所以我们经常就会假设它服从好几个高斯分布的加权和，因为至少我们从几何的角度看它是比较合理的。顺理成章的，我们可以假设
$$
X \sim \sum\limits_{k=1}^{K}\pi_kN(\mu_k,\Sigma_k)
$$
其中:
$$
\sum\limits_{k=1}^{K}\pi_k = 1
$$


### 聚类问题

​	2.2节从几何和概率论的角度建立了高斯混合模型，接下来是从和我专业相关的机器学习的角度入手，介绍高斯混合模型在无监督学习中的应用。

聚类问题是一类无监督学习问题，也就是我们并不知道每一个样本它的标签，我们需要通过算法去估计它的类别，所以我们可以设它的类别为一个离散的随机变量(隐变量)。现在我们知道某一个样本的数据(特征)$x$,那我们想知道它属于类别$y$ 的概率是多大。

那高斯混合模型是如何进行分类的呢？这就和我们提到的多个高斯分布相关了，从机器学习的角度来看，我们一开始就假定每一个类别中的样本都是服从高斯分布的，这很合理，接着我们通过具体的数据去确定这个分布中的参数，这个过程可以理解为模型在学习这个类别的特征。学习完成以后，随机给出一个样本时，模型会判断它属于某个类别的概率是多少，而我们一般会取概率最大的那个。如果设样本所属的类别为离散随机变量$Z$(隐变量)，则高斯混合模型对这个样本会做出以下判断

| Z    | $C_1$ | $C_2$ | $\cdots$ | $C_K$ |
| ---- | ----- | ----- | -------- | ----- |
| $p$  | $p_1$ | $p_2$ | $\cdots$ | $p_K$ |

补充：隐变量是一类不能被直接观察，但是对模型的输出又有影响的随机变量，在这里是$Z$



### 模型求解

随机变量$X$的pdf为
$$
f(x) = \sum\limits_{k=1}^K\pi_k\frac{1}{(2\pi)^\frac{d}{2}|\Sigma_k|^\frac{1}{2}}e^{-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)} \tag{2.1}
$$
我们需要使用样本数据$x^{(i)}$来求解所有的参数$\theta = {\mu_k,\Sigma_k,\pi_k}$



#### 极大似然估计

​	在概率论中，我们经常使用极大似然估计来确定参数。它的核心逻辑是让已经观测到的现象发生的概率最大，这是很自然的事情。也就是它会让所有被观测到的样本的数据发生的概率最大。因为所有的样本都是独立，所以我们只需要将所有样本被观测到的概率相乘即可
$$
Max P(X)\\
P(X) = \prod_{i=1}^{m}P(x^{(i)}) \tag{2.2}
$$
这里会遇到问题，因为对于连续随机变量而言，任何一点的概率都是零，那么$P(\mathcal{X})$也应该是零。为了避免它，我们通常会取一个较小的区间来代替这个点，避免它的概率为零。

结合(2.1)和(2.2)，我们可以得到$P(\mathcal{X})$关于参数$\theta$的函数，我们需要做的就是
$$
\hat{\theta} = \mathop{argmax}_{\theta}(P({X})) \tag{2.3}
$$
我们可以将$P(\mathcal{X})$分别对$\mu_k ,\Sigma_k,\pi_k$求偏导，接着解出最优的$\theta$

但遗憾的是：由于(2.2)式过于复杂，我们并不能求出解析解



#### EM算法

​	当方程无法直接求解时，我们可以通过一些特定的算法来逼近最佳参数。例如：梯度下降算法、牛顿法等等。同样的，这里我们使用EM算法来不断优化参数。

在EM算法中我们需要引入隐变量，需要从混合模型的角度下看待高斯混合模型的概率分布

这里使用$Z = (z_1, z_2, \dots, z_N)$ 代表隐变量
$$
\begin{flalign}
&P(X)\\
&=\int_{Z}P(X,Z)dz\\
&=\sum\limits_ZP(X,Z=C_k)\\
&=\sum\limits_{k=1}^{K}P(Z=C_k)P(X|Z=C_k)\\
&=\sum\limits_{k=1}^Kp_kN(X|\mu_k,\sigma^2_k)\\
&=\sum\limits_{k=1}^{K}p_k\phi(x|\theta_k)

\end{flalign}
$$
EM算法通过迭代求$L(\theta) = logP(X|\theta)$的极大似然估计，每次迭代包含两步:E步(求期望)，M步(极大化)

伪代码形式:

+ 输入:观测数据$X$，隐变量$Z$

+ 步骤

+ 1. 初始化参数$\theta_0$,迭代

  2. E步:记$\theta^t$为第t次迭代参数$\theta$的估计值，在第$i+1$次，计算

  3. $$
     \begin{flalign}
     Q(\theta, \theta^t)
     &= E_Z[logP(X,Z|\theta)|X,\theta^{(t)}]\\
     &=\sum\limits_ZlogP(X,Z|\theta)P(Z|X,\theta^{(t)})
     \end{flalign}
     $$

  4. M步:求使得$Q(\theta,\theta^{(t)})$极大的$\theta$,确定第$t+1$次迭代的参数估计值$\theta^{t+1}$

  5. 重复，直到收敛

具体的推导太复杂，所以省略了



## 案例应用

​     下面我将使用GMM模型对lris数据集中的三种花进行分类，为了方便可视化，每个样本只选取了两个特征:feature1、feature2。我们训练模型时，模型并不知道每朵花的类别，当我们训练好以后我们也只能使用无监督学习的评估方法来评估这个模型的好坏。

下面是模型预测结果和真实标签的比较图

<img src="https://raw.githubusercontent.com/liangruogu/BlogImages/main/img/image-20241208212304223.png" style="transform:scale(60%)">

可以看出蓝色的标签因为和红色和绿色分割的很明显，所以它的分类是最好的，相比之下，红色和绿色之间的界限很模糊，甚至它们两个都是夹杂在一起，所以分类起来较为困难。

这是无监督学习的评价指标--ARI 它的取值范围是-1到1,

Adjusted Rand Index (ARI): 0.5548493819740281



## 总结

​	本文详细介绍了高斯混合模型，并使用它完成了一个聚类问题。我们不难看出概率论在整个过程中起到了关键作用，因为模型最开始的假设就依赖于中心极限定理，而且模型求解也使用到了概率论中的极大似然估计。在最后的案例中GMM取得了比较好的结果，这也印证了假设的正确性。

​	概率论在机器学习中的应用远不止于此，朴素贝叶斯、隐马尔可夫链等等都和机器学习中的垃圾邮件分类、天气预测、NLP等都紧密相关，选择GMM的原因是它包含了概率论中非常核心的定理--中心极限定理，并且在求解过程中也使用到了极大似然估计。不局限于机器学习，概率论在随机过程、赌博、博弈论、量子力学中都发挥了不可或缺的作用。



学习资料:

[基于GMM和SVM的音频分类算法 - 豆丁网](https://www.docin.com/p-302942293.html)

[机器学习-白板推导系列(十一)-高斯混合模型GMM(Gaussian Mixture Model)](https://www.bilibili.com/video/BV13b411w7Xj/?spm_id_from=333.337.search-card.all.click&vd_source=625f2432f18c97da1886f1da2e89d33f)